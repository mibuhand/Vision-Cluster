#!/usr/bin/env python3
"""
Image Clustering Script

This script loads the CLS token embeddings generated by dinov3_embeddings.py
and performs similarity-based clustering using K-means or hierarchical clustering.
The number of clusters is adjustable to control granularity.

Usage:
    python cluster_images.py [--clusters N] [--method kmeans|hierarchical] [--output-dir DIR]
"""

import numpy as np
import argparse
import logging
from pathlib import Path
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import shutil
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_embeddings(embeddings_path: Path) -> tuple:
    """
    Load embeddings and image names from the .npz file.
    
    Args:
        embeddings_path: Path to the embeddings .npz file
        
    Returns:
        Tuple of (embeddings array, image names list)
    """
    if not embeddings_path.exists():
        raise FileNotFoundError(f"Embeddings file not found: {embeddings_path}")
    
    data = np.load(embeddings_path)
    embeddings = data['cls_tokens']
    image_names = data['image_names']
    
    logging.info(f"Loaded {len(embeddings)} embeddings with dimension {embeddings.shape[1]}")
    return embeddings, image_names

def perform_clustering(embeddings: np.ndarray, n_clusters: int, method: str = 'kmeans') -> np.ndarray:
    """
    Perform clustering on the embeddings.
    
    Args:
        embeddings: Array of embeddings
        n_clusters: Number of clusters
        method: Clustering method ('kmeans' or 'hierarchical')
        
    Returns:
        Array of cluster labels
    """
    # Normalize embeddings for better clustering
    scaler = StandardScaler()
    embeddings_normalized = scaler.fit_transform(embeddings)
    
    if method == 'kmeans':
        clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    elif method == 'hierarchical':
        clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    else:
        raise ValueError(f"Unknown clustering method: {method}")
    
    logging.info(f"Performing {method} clustering with {n_clusters} clusters...")
    labels = clusterer.fit_predict(embeddings_normalized)
    
    # Calculate silhouette score for quality assessment
    if len(set(labels)) > 1:  # Need at least 2 clusters for silhouette score
        silhouette_avg = silhouette_score(embeddings_normalized, labels)
        logging.info(f"Silhouette score: {silhouette_avg:.3f}")
    
    return labels

def organize_images_by_clusters(image_names: list, labels: np.ndarray, 
                              processed_dir: Path, output_dir: Path):
    """
    Organize images into cluster directories.
    
    Args:
        image_names: List of image names
        labels: Cluster labels for each image
        processed_dir: Directory containing the processed images
        output_dir: Directory to create cluster subdirectories
    """
    # Remove existing clustered data to avoid pollution
    if output_dir.exists():
        logging.info(f"Removing existing clustered data: {output_dir}")
        shutil.rmtree(output_dir)
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Group images by cluster
    cluster_counts = {}
    for i, (image_name, cluster_id) in enumerate(zip(image_names, labels)):
        cluster_dir = output_dir / f"cluster_{cluster_id:03d}"
        cluster_dir.mkdir(exist_ok=True)
        
        # Find the source image file
        source_path = processed_dir / image_name
        if source_path.exists():
            dest_path = cluster_dir / image_name
            shutil.copy2(source_path, dest_path)
            
            # Count images per cluster
            cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1
        else:
            logging.warning(f"Source image not found: {source_path}")
    
    # Log cluster statistics
    logging.info("Cluster distribution:")
    for cluster_id in sorted(cluster_counts.keys()):
        count = cluster_counts[cluster_id]
        logging.info(f"  Cluster {cluster_id:3d}: {count:3d} images")
    
    return cluster_counts

def suggest_optimal_clusters(embeddings: np.ndarray, max_clusters: int = 20) -> int:
    """
    Suggest optimal number of clusters using elbow method.
    
    Args:
        embeddings: Array of embeddings
        max_clusters: Maximum number of clusters to test
        
    Returns:
        Suggested number of clusters
    """
    scaler = StandardScaler()
    embeddings_normalized = scaler.fit_transform(embeddings)
    
    max_test_clusters = min(max_clusters, len(embeddings) - 1)
    inertias = []
    
    logging.info("Analyzing optimal cluster count...")
    for k in range(2, max_test_clusters + 1):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(embeddings_normalized)
        inertias.append(kmeans.inertia_)
    
    # Simple elbow detection: find largest drop in inertia
    drops = [inertias[i] - inertias[i+1] for i in range(len(inertias)-1)]
    optimal_k = drops.index(max(drops)) + 3  # +3 because we started from k=2
    
    logging.info(f"Suggested number of clusters: {optimal_k}")
    return optimal_k

def main():
    """Main execution function."""
    # Directory setup
    proj_dir = Path(__file__).resolve().parent.parent
    processed_dir = proj_dir / 'data' / 'processed'
    embeddings_path = proj_dir / 'data' / 'cls_tokens.npz'
    
    parser = argparse.ArgumentParser(description='Cluster images based on DINOv3 embeddings')
    parser.add_argument('--clusters', '-k', type=int, default=0,
                        help='Number of clusters (0 = auto-detect optimal)')
    parser.add_argument('--method', choices=['kmeans', 'hierarchical'], default='kmeans',
                        help='Clustering method (default: kmeans)')
    parser.add_argument('--output-dir', default=str(proj_dir / 'data' / 'clustered'),
                        help='Output directory for clustered images')
    
    args = parser.parse_args()
    
    # Load embeddings
    try:
        embeddings, image_names = load_embeddings(embeddings_path)
    except FileNotFoundError as e:
        logging.error(str(e))
        logging.error("Please run dinov3_embeddings.py first to generate embeddings")
        return
    
    # Determine number of clusters
    if args.clusters == 0:
        n_clusters = suggest_optimal_clusters(embeddings)
    else:
        n_clusters = args.clusters
    
    if n_clusters >= len(embeddings):
        logging.error(f"Number of clusters ({n_clusters}) must be less than number of images ({len(embeddings)})")
        return
    
    # Perform clustering
    labels = perform_clustering(embeddings, n_clusters, args.method)
    
    # Organize images by clusters
    output_dir = Path(args.output_dir)
    cluster_counts = organize_images_by_clusters(image_names, labels, processed_dir, output_dir)
    
    logging.info(f"Clustering complete! Images organized into {len(cluster_counts)} clusters in: {output_dir}")

if __name__ == "__main__":
    main()